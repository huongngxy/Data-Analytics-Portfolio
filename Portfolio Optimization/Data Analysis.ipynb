{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac4523f-4703-41d4-9c81-5b9cb043103b",
   "metadata": {},
   "source": [
    "# Portfolio Optimization with the Mean-Variance Model <br> <br> Part 2: Data Analysis & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798171a-052d-4406-82bc-0a072fe8ac23",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a> \n",
    "## Table of Contents\n",
    "1. [Data Analysis](#Data-Analysis)\n",
    "   - [1.1. Return](#1.2.-Return)\n",
    "   - [1.2. Pearson Correlation](#1.3.-Pearson-Correlation)\n",
    "   - [1.3. Stock Return Statistics](#1.4.-Stock-Return-Statistics)\n",
    "   - [1.4. Efficient Frontier](#1.5.-Efficient-Frontier)\n",
    "2. [Results](#2.-Results)\n",
    "   - [2.1. MV Optimization](#2.1.-MV-Optimization)\n",
    "   - [2.2. Performance Evaluation](#2.2.-Performance-Evaluation)\n",
    "     - [Market Sensitivity](#Market-Sensitivity)\n",
    "     - [Cumulative Returns](#Cumulative-Returns)\n",
    "     - [Sharpe Ratio](#Sharpe-Ratio)\n",
    "     - [Sortino Ratio](#Sortino-Ratio)\n",
    "     - [Drawdown](#Drawdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25878162-05e1-4b7e-bc36-a99f4bfec98d",
   "metadata": {},
   "source": [
    "## 1. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd2bc1-3dda-4922-808c-449645b49358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import skew,kurtosis\n",
    "import cvxpy as cp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfcfc7-6a19-4e81-827a-040aede66e30",
   "metadata": {},
   "source": [
    "### 1.1. Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e438f-ba7c-42f7-b9a6-e682c0359afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import equity prices  \n",
    "price_df = pd.read_csv('stock prices.csv',decimal = ',', index_col = 0)\n",
    "# compute returns\n",
    "return_df = price_df.pct_change().dropna()\n",
    "expected_return = return_df.mean()   # vector of means \n",
    "return_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8c19e-fd6c-4310-b48b-4a0aa7f77e20",
   "metadata": {},
   "source": [
    "### 1.3. Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96146084-8013-4eb2-ba1d-eb624f14a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot correlation matrix \n",
    "Correlation = return_df.corr(method = 'pearson')\n",
    "px.imshow(Correlation, title = 'Figure 1: Correlation Matrix of Returns', height = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7232d98-8406-4001-9eeb-f8d6137ffd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot covariance matrix \n",
    "covariance_df = return_df.cov(ddof = 0)  # population covariance with no degree of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5657c-6f60-4506-9828-dc033aa30a03",
   "metadata": {},
   "source": [
    "### 1.4. Stock Return Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761b0f3-2906-4c48-805d-4a2d1d925b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate return statistics (mean, variance, skewness and excess kurtosis)\n",
    "def statistics(df):\n",
    "    mean_df = df.mean().to_frame(name = 'mean').reset_index()\n",
    "    std_df = df.std(ddof = 0).to_frame(name = \"std\").reset_index()   # biased std \n",
    "    skew_df = pd.Series(skew(df, bias = True)).to_frame(name = 'skew')  # biased skew \n",
    "    kurt_df = pd.Series(kurtosis(df, fisher = True, bias = True)).to_frame(name = 'kurt') # biased excess kurtosis \n",
    "    min_value = df.min().to_frame(name = 'min').reset_index() # min\n",
    "    median = df.median().to_frame(name = 'median').reset_index() # median\n",
    "    max_value = df.max().to_frame(name = 'max').reset_index() #max \n",
    "    # create a dataframe containing computed measures \n",
    "    stats_df = pd.concat([mean_df,std_df, skew_df, kurt_df, min_value, median, max_value], ignore_index = False, axis = 1)\n",
    "    stats_df1 = stats_df.set_index(stats_df.iloc[:,0]).drop('index', axis = 1)\n",
    "    return stats_df1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ba26b-dcfc-49c5-ba70-5f5650780e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistical properties for all stocks\n",
    "Stats = statistics(return_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e285c38-f26e-4e2d-bc84-d8aa07b5ccaa",
   "metadata": {},
   "source": [
    "#### Stocks with the Highest Mean Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01030c-7289-4e09-977f-04a0391749aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort stocks by return and choose 5 with the highest returns\n",
    "mean_sorted = Stats.T.sort_values(by = 'mean', ascending = False).head(5)\n",
    "mean_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c2f3d-b015-47cd-bdf2-da7274ca9dab",
   "metadata": {},
   "source": [
    "#### Stocks with the Lowest Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36980e7e-2851-46bb-adfc-4c3f6881dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort stocks by standard deviation and choose 5 with the lowest values\n",
    "std_sorted = Stats.T.sort_values(by = 'std', ascending = False).tail(5)\n",
    "std_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78d9b5-9442-4aac-a686-e193fa98f8d0",
   "metadata": {},
   "source": [
    "#### Stocks with the Highest Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d58d57-2d6d-4303-b384-0f7eae80477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort stocks by skewness and choose 5 with the highest values \n",
    "sk_sorted = Stats.T.sort_values(by = 'skew', ascending = False).head(5)\n",
    "sk_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d8d1e-92eb-4f52-817f-771a6144c356",
   "metadata": {},
   "source": [
    "#### Stocks with the Lowest Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a9d2e-1a6e-4791-99ba-06143bad0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort stocks and choose 5 with the lowest values \n",
    "kurt_sorted = Stats.T.sort_values(by = 'kurt', ascending = False).tail(5)\n",
    "kurt_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f81a0-83dd-4e22-882f-8199cfeca0d2",
   "metadata": {},
   "source": [
    "### 1.5. Efficient Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7692d5-3f67-4994-8ad4-99ff887822c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of assets equals the length of the mean vector \n",
    "n_assets = len(expected_return)\n",
    "\n",
    "# define a function to compute feasible efficient portfolios \n",
    "def efficient_frontier_cvx(mu, Sigma, n_points=50, short_sales=False):\n",
    "    n = len(mu)\n",
    "    w = cp.Variable(n)\n",
    "    mu_target = cp.Parameter()\n",
    "\n",
    "    constraints = [cp.sum(w) == 1]\n",
    "    if not short_sales:\n",
    "        constraints.append(w >= 0)\n",
    "\n",
    "    frontier_returns, frontier_vols, frontier_weights = [], [], []\n",
    "\n",
    "    for target in np.linspace(mu.min(), mu.max(), n_points):\n",
    "        mu_target.value = target\n",
    "        prob = cp.Problem(cp.Minimize(cp.quad_form(w, Sigma)),\n",
    "                          constraints + [mu @ w == mu_target])\n",
    "        prob.solve(solver=cp.SCS, verbose=False)\n",
    "\n",
    "        if w.value is not None:\n",
    "            ret = mu @ w.value\n",
    "            vol = np.sqrt(w.value.T @ Sigma @ w.value)\n",
    "            frontier_returns.append(ret)\n",
    "            frontier_vols.append(vol)\n",
    "            frontier_weights.append(w.value)\n",
    "\n",
    "    return np.array(frontier_returns), np.array(frontier_vols), np.array(frontier_weights)\n",
    "\n",
    "\n",
    "# apply the function \n",
    "opt_rets, opt_vols, ef_optw = efficient_frontier_cvx(expected_return.values, covariance_df.values, n_points=50, short_sales=False) \n",
    "\n",
    "# Compute the Minimum Variance Portfolio (MVP)\n",
    "ef_gmv = cp.Variable(n_assets)\n",
    "ef_prob = cp.Problem(cp.Minimize(cp.quad_form(ef_gmv,covariance_df)),\n",
    "                    [cp.sum(ef_gmv) == 1, ef_gmv >= 0])\n",
    "\n",
    "ef_prob.solve(solver = cp.OSQP, verbose = False)\n",
    "ef_gmv_w = ef_gmv.value  # asset allocations of the MVP \n",
    "ef_gmv_ret = float(expected_return @ ef_gmv_w) # expected return of the MVP \n",
    "ef_gmv_vol = float(np.sqrt(ef_gmv_w @ covariance_df @ ef_gmv_w)) # volatility of the MVP\n",
    "\n",
    "# Identify the GMV index on the frontier \n",
    "ef_gmv_idx = np.argmin(opt_vols) \n",
    "vols_below = opt_vols[:ef_gmv_idx+1]\n",
    "rets_below = opt_rets[:ef_gmv_idx+1]\n",
    "\n",
    "vols_above = opt_vols[ef_gmv_idx:]\n",
    "rets_above = opt_rets[ef_gmv_idx:]\n",
    "\n",
    "# Split frontier into efficient/inefficient\n",
    "vols_below, rets_below = opt_vols[:ef_gmv_idx+1], opt_rets[:ef_gmv_idx+1]\n",
    "vols_above, rets_above = opt_vols[ef_gmv_idx:], opt_rets[ef_gmv_idx:]\n",
    "\n",
    "# Last point on the inefficient frontier\n",
    "ineff_slice = slice(0, ef_gmv_idx + 1)\n",
    "\n",
    "# Within that slice, find the index of the highest volatility\n",
    "local_idx = int(np.argmax(opt_vols[ineff_slice]))\n",
    "idx_max_vol_ineff = local_idx  # because slice starts at 0\n",
    "\n",
    "# Extract weights and show as a Series\n",
    "w_max_vol_ineff = ef_optw[idx_max_vol_ineff]\n",
    "components_max_vol_ineff = pd.Series(w_max_vol_ineff, index=return_df.columns)\n",
    "\n",
    "# Last point on the efficient frontier \n",
    "eff_slice = slice(ef_gmv_idx, len(opt_vols))\n",
    "eff_local_idx = int(np.argmax(opt_vols[eff_slice]))\n",
    "idx_max_vol_eff = ef_gmv_idx + eff_local_idx\n",
    "w_max_vol_eff = ef_optw[idx_max_vol_eff]\n",
    "compo_max_vol_eff = pd.Series(w_max_vol_eff, index = return_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8b954-adc1-40c4-a747-ca46fe88aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the efficient frontier\n",
    "fig = go.Figure()\n",
    "\n",
    "# Efficient Frontier \n",
    "fig.add_trace(go.Scatter(\n",
    "    x = vols_above, y=rets_above,\n",
    "    mode = 'lines',\n",
    "    line = dict(color='red', dash='solid', width=2),\n",
    "    name = 'Constrained efficient frontier'\n",
    "))\n",
    "\n",
    "# Inefficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = vols_below, y=rets_below,\n",
    "    mode = 'lines',\n",
    "    line = dict(color='red', dash='dash', width=2),\n",
    "    name = 'Constrained inefficient frontier'\n",
    "))\n",
    "\n",
    "# GMV point \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[ef_gmv_vol], y=[ef_gmv_ret],\n",
    "    mode='markers',\n",
    "    marker=dict(color='red', size=10, symbol='circle'),\n",
    "    name='Constrained MVP'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = [(return_df @ compo_max_vol_eff).std(ddof = 0)], y = [(return_df @ compo_max_vol_eff).mean()],\n",
    "    mode = 'markers',\n",
    "    marker = dict(color = 'red', size = 13, symbol = 'pentagon'),\n",
    "    name = '100% invested in Sartorius'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = [(return_df @ components_max_vol_ineff).std(ddof = 0)], y = [(return_df @ components_max_vol_ineff).mean()],\n",
    "    mode = 'markers',\n",
    "    marker = dict(color = 'red', size = 10, symbol = 'square'),\n",
    "    name = '100% invested in Heidelberger Druckmaschinen'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Figure 2: Efficient Frontier',\n",
    "    xaxis_title='Volatility (%)',\n",
    "    yaxis_title='Expected return (%)',\n",
    "    template='plotly_white',\n",
    "    legend=dict(x=0.8, y=1),\n",
    "    width = 950, height = 550\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7282ce1-ce96-4f6d-8ea6-536e1dcf3444",
   "metadata": {},
   "source": [
    "## 2. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49f268-ffb7-4140-8b30-544e56b921bf",
   "metadata": {},
   "source": [
    "### 2.1. MV Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96e451-230c-407f-b928-11295f8f726c",
   "metadata": {},
   "source": [
    "The optimal portfolio allocation is found by maximizing the expected portfolio return and minimizing the portfolio variance via the following **objective function**: <br> \n",
    "$$ \\min_{\\mathbf{w}} f(\\mathbf{w})\n",
    "= -\\lambda_1 \\mathbf{w}^\\top \\boldsymbol{\\mu}\n",
    "+ \\lambda_2 \\mathbf{w}^\\top \\boldsymbol{\\Sigma} \\mathbf{w}\n",
    "$$\n",
    "\n",
    "**subject to**\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\ge 0, \\quad \\sum_{i=1}^{n} w_i = 1\n",
    "$$\n",
    "\n",
    "**where:**\n",
    "- $\\mathbf{w}$ is the $n \\times 1$ vector of portfolio weights  \n",
    "- $\\boldsymbol{\\mu}$ is the $n \\times 1$ vector of expected stock returns  \n",
    "- $\\boldsymbol{\\Sigma}$ is the $n \\times n$ covariance matrix of stock returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc49dd9-6bd3-4061-88fd-7035b596828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization function for the Mean-Variance (MV) Model\n",
    "def opt_funct(df,lambda_1 = None, lambda_2 = None):\n",
    "    pf_mu = df.mean()\n",
    "    pf_sigma = df.cov(ddof = 0).values\n",
    "    w = cp.Variable(len(pf_mu))\n",
    "    pf_return = w @ pf_mu\n",
    "    pf_variance = w.T @ pf_sigma @ w\n",
    "    objective_funct = cp.Minimize(-lambda_1 * pf_return + lambda_2 * pf_variance)   # objective function for MV \n",
    "    constraints = [cp.sum(w) == 1, w >= 0]   # restrictions on long positions \n",
    "    problem = cp.Problem(objective_funct, constraints)\n",
    "    problem.solve(solver = cp.OSQP, verbose  = False)\n",
    "    opt_w = w.value.round(5)\n",
    "    pf_daily_return = pf_mu @ opt_w   # daily portfolio return \n",
    "    pf_return_annualized = ((pf_daily_return+1)**255)-1  # annualized portfolio return; 255 is the average number of annual trading days based on the dataset\n",
    "    pf_daily_vol = np.sqrt(opt_w.T @ pf_sigma @ opt_w)   # daily portfolio volatility \n",
    "    pf_vol_annualized = pf_daily_vol * np.sqrt(255)  # annualized portfolio volatility \n",
    "    short_sales = []\n",
    "    for i in opt_w:\n",
    "        if i < 0: \n",
    "            short_sales.append(i)\n",
    "    return (opt_w, \n",
    "            problem.value, \n",
    "            pf_daily_return, \n",
    "            pf_return_annualized, \n",
    "            pf_daily_vol, \n",
    "            pf_vol_annualized, \n",
    "            sum(abs(opt_w)),\n",
    "            sum(short_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ade76-2a90-4a1b-87bc-8a0ec45adde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for a plot with highlights\n",
    "def highlight_plot(highlight_col, df, plot_title):\n",
    "    # assign distinct colors for highlighted cols\n",
    "    highlight_colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\"]\n",
    "    color_map = {col: highlight_colors[i % len(highlight_colors)]\n",
    "                 for i, col in enumerate(highlight_col)}\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for col in df.columns:\n",
    "        if col in highlight_col:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x = df.index,\n",
    "                    y = df[col],\n",
    "                    mode = 'lines',\n",
    "                    name = col,\n",
    "                    line = dict(width=2, color=color_map[col]),\n",
    "                    opacity = 1, showlegend = True\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = df.index,\n",
    "                    y = df[col],\n",
    "                    mode = 'lines',\n",
    "                    name = col,\n",
    "                    line = dict(width=1, color=\"gray\"),\n",
    "                    opacity = 0.3, showlegend = False\n",
    "                )\n",
    "            )\n",
    "    fig.update_layout(\n",
    "        height = 600, \n",
    "        template = 'plotly_white',\n",
    "        xaxis_title = 'Date',\n",
    "        yaxis_title = 'Portfolio value, indexed to 1',\n",
    "        legend = dict(x = 0.9, y = 1.01),\n",
    "        title = plot_title \n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541decc1-2c09-4f59-bf45-75b30ca44a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization with only long positions allowed \n",
    "MV_LP = opt_funct(return_df, lambda_1 = 1, lambda_2 = 1) \n",
    "print(\"Optimal Weights:\", MV_LP[0])\n",
    "print(\"Function Value:\", MV_LP[1])\n",
    "print(\"Daily Return:\", MV_LP[2])\n",
    "print(\"Annualized Return:\", MV_LP[3])\n",
    "print(\"Daily Volatility:\", MV_LP[4])\n",
    "print(\"Annualized Volatility:\", MV_LP[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa17fc-b9d9-4dc1-8556-bb6d65a52c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the optimized weights\n",
    "ticker_df = expected_return.index.to_frame(name  = 'Stock',index = False)\n",
    "MV_w_LP = pd.Series(MV_LP[0]).to_frame(name = 'Weight')\n",
    "MV_LP_df = pd.concat([ticker_df, MV_w_LP], axis = 1).sort_values(by = 'Weight', ascending = False)\n",
    "MV_LP_df = MV_LP_df.loc[MV_LP_df['Weight'] > 0]\n",
    "\n",
    "# plot optimized portfolio composition\n",
    "px.pie(MV_LP_df, values = \"Weight\", names = \"Stock\", height = 500, title = \"Figure 3: Optimized MV-Portfolio Weights\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae8fd2-b3b7-431a-8e2d-66178e27c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "MV_ret_LP = (return_df @ MV_LP[0]).to_frame()\n",
    "statistics(MV_ret_LP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284fd44-4c5d-4b23-b7da-2ce4a7c72578",
   "metadata": {},
   "source": [
    "### 2.2. Performance Evaluation\n",
    "The following sections evaluate the computed MV-efficient portfolio by looking at its **sensitivity to the market benchmark ($\\beta $)**, **diversification degree ($R^2 $)**,  **cumulative return**, **Sharpe and Sortino ratios** as well as its **drawdowns** throughout the entire observation period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33efd27-a38f-43cf-9ccf-12561e0f4af9",
   "metadata": {},
   "source": [
    "#### Market Sensitivity\n",
    "\n",
    "To begin with, portfolio variance can be decomposed as:\n",
    "$$\\text{Total portfolio risk} = \\text{Systematic risk} + \\text{Idiosyncratic risk}$$\n",
    "which is equivalent to: \n",
    "$$\\sigma_p^2 = \\beta_p^2 \\sigma_m^2 + \\sigma_{\\varepsilon,p}^2 \\tag{21} $$\n",
    "\n",
    "Accordingly, the relative contributions of systematic and idiosyncratic risk to the portfolio‚Äôs total risk are given by:\n",
    "\n",
    "$$ \\text{Systematic risk share} = R^2 = \\frac{\\beta_p^2 \\sigma_m^2}{\\sigma_p^2}\\tag{22}$$\n",
    "\n",
    "$$\\text{Idiosyncratic risk share} =1- R^2 = 1 -\\frac{\\beta_p^2 \\sigma_m^2}{\\sigma_p^2}\\tag{23}$$\n",
    "\n",
    "To determine the portfolio‚Äôs systematic risk, the DAX Performance Index was used as a proxy for the overall market. This risk component is governed by the factor $\\beta$ and is not affected by diversification. Idiosyncratic risk, in contrast, is diversifiable and diminishes as assets are added to the portfolio. That is, a well-diversified portfolio should contain less idiosyncratic risk than systematic risk.<br>\n",
    "\n",
    "$\\beta$ is a constant quantifying the sensitivity of the portfolio return $R_{\\text{p}}$ to changes in the market return $R_{\\text{m}}$. A value of $\\beta$ implies a procyclical behavior that the expected portfolio return increases (decreases) by 2% if the market return increases (decreases) by 1%. Likewise, a value of $\\beta$ = 1 suggests that the portfolio moves in lockstep with the market, while a $\\beta$ < 1 reflects dampened sensitivity to market fluctuations. The $\\beta$ of any portfolio is calculated as the standardized covariance between the portfolio return $R_{\\text{p}}$ and the market return $R_{\\text{m}}$: \n",
    "$$\\beta = \\frac{\\sigma_{p,m}}{\\sigma_m^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9b599-f74d-4ab4-af45-b725192303d4",
   "metadata": {},
   "source": [
    "##### Explorative Analysis of DAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a1b5e-dcda-4c22-adec-0c419ba37fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dax prices and compute returns to compare with the results of the MV Model \n",
    "DAX_idx = pd.read_excel('dax index.xlsx', index_col = 0)\n",
    "return_df.index = pd.to_datetime(return_df.index, format = 'mixed', dayfirst = True, errors = 'coerce')\n",
    "MV_ret_LP.index = pd.to_datetime(MV_ret_LP.index, format = 'mixed', dayfirst = True, errors = 'coerce')\n",
    "DAX_idx = DAX_idx.rename(columns = {DAX_idx.columns[0]:'DAX'}).pct_change().dropna()\n",
    "Dax_and_MV = pd.merge(DAX_idx,MV_ret_LP.rename(columns = {0:'MV'}), left_index = True, right_index = True, how = 'left').dropna()\n",
    "merge_all = pd.merge(Dax_and_MV, return_df, left_index = True, right_index = True, how = 'left').dropna()\n",
    "market_return = Dax_and_MV.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e79368a-2f81-41db-acb0-32a1b6095517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics on DAX performance \n",
    "statistics(market_return.to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1dd92-eb90-42ac-a4f2-619f7ea99ad3",
   "metadata": {},
   "source": [
    "##### Systematic Risk, Idiosyncratic Risk and Beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3e414-e057-4241-9287-9edcc931977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate beta of the MV portfolio\n",
    "R_squared = {}\n",
    "\n",
    "# Loop through all columns in Dax_indvret\n",
    "for col in merge_all.columns[1:]:\n",
    "    y = merge_all[col]\n",
    "    x = merge_all.iloc[:, 0]\n",
    "    r_value = stats.linregress(x, y)\n",
    "    R_squared[col] = [r_value.rvalue ** 2, r_value.slope, r_value.pvalue]\n",
    "\n",
    "\n",
    "# Convert dict to DataFrame\n",
    "R2_df = pd.DataFrame.from_dict(R_squared, orient='index', columns=['R¬≤', 'Beta', 'p-value'])\n",
    "R2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e69fc5-9782-4fc1-a1ef-0e59a8f8f491",
   "metadata": {},
   "source": [
    "Note: The MV-efficient portfolio is mainly driven by firm-specific risk, as evidenced by its low R-squared. Furthermore, the portfolio exhibits a moderate sensitivity to the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c4f61-11ce-4485-b54b-97df46bb35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(R2_df.iloc[1:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565cd6f-861a-40d9-b117-a0e6b973c253",
   "metadata": {},
   "source": [
    "Note: Compared to individual stocks, the optimized portfolio is less dependent on firm-specific risk and less sensitive to market changes on average. These computed measures appear to be significant across all stocks as well as the portfolio, as indicated by the p-value approaching 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98e824-8d07-4218-a433-84466f32a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(Dax_and_MV, x= \"DAX\", y='MV', trendline=\"ols\",\n",
    "           height = 500, title = \"Figure 4: Beta of MV-efficient Portfolio\", template = 'plotly_white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f04ff5-8973-4ae3-941a-1913badbfb7f",
   "metadata": {},
   "source": [
    "#### Cumulative Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c16e0-da99-4504-b485-2abfcdbc695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cumulative returns for the benchmark (DAX) and the MV portfolio\n",
    "ret_cum = (merge_all+1).cumprod()\n",
    "\n",
    "# cumulative returns with those of the MV portfolio and DAX hightlighted\n",
    "highlight_plot(highlight_col = ['MV', 'DAX'], df=ret_cum, plot_title = \"Figure 4: Cumulative Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba7128-8f17-4251-a7ef-48a646f4280b",
   "metadata": {},
   "source": [
    "Note: The optimized MV portfolio achieves substantially higher terminal wealth than DAX and any  individual stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8505017-20af-4ea7-9442-ec95ea94a0fe",
   "metadata": {},
   "source": [
    "#### Risk-adjusted Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c69393-bdde-4e6b-9ee2-f5357319461d",
   "metadata": {},
   "source": [
    "The term \"Risk-adjusted performance\" refers to the Sharpe ratio and the Sortino ratio.\n",
    "\n",
    "**Sharpe Ratio** <br>\n",
    "The Sharpe ratio, developed by William F. Sharpe, is one of the most widely used perfor\n",
    "mance metrics for measuring the excess return of an investment above the risk-free rate per \n",
    "unit of risk. This risk-adjusted ratio takes the formula:  \n",
    "$$ \\text{Sharpe Ratio} = \\frac{\\mu_p - R_p}{\\sigma_p} $$\n",
    " \n",
    "where $ùëÖ_ùëì$ denotes the risk-free rate. Higher values signify stronger risk-adjusted performance, as it means the portfolio generates higher excess return for every unit of risk taken. $ùëÖ_ùëì$ was proxied by the ECB main refinancing operations rate. \n",
    "\n",
    "**Sortino Ratio** <br>\n",
    "The Sortino ratio is a variation of the Sharpe ratio, developed from the realization that not all volatility is undesirable. Investors are in fact concerned with the risk of losses rather than with the risk of unexpectedly high gains. While the Sharpe ratio penalizes both upside and downside risk, the Sortino ratio focuses only on the downside risk, also known as semideviation $\\sigma_ùëë$. This metric measures the volatility of returns that fall below a specified minimum acceptable return $MAR$. The formulas for the Sortino ratio and portfolio semi-deviation are:  \n",
    "$$ \\text{Sortino Ratio} = \\frac{\\mu_p - MAR}{\\sigma_d} $$\n",
    "$$\\sigma_d = \\sqrt{\\sigma_d^2} = \\sqrt{\\frac{1}{N} \\sum_{t=1}^{N} \\left[ \\min(0, R_{p,t} -MAR) \\right]^2 }$$\n",
    "A higher Sortino ratio is preferable, as it indicates that the investment generates more excess return for every unit of ‚Äúbad‚Äù risk it takes on. For the analysis, the $MAR$ was set equal to the risk-free rate $ùëÖ_ùëì$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325c279-c9c4-418a-b5f4-f47e01deebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import riskfree rates (ECB main refinanancing operations rates)\n",
    "riskfree_df = pd.read_excel('leitzins.xlsx').set_index('Date')\n",
    "riskfree_df.index = pd.to_datetime(riskfree_df.index, format = 'mixed', dayfirst = True, errors = 'coerce')\n",
    "riskfree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a428b4-2d0f-4874-81e8-5bcb897fe3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that contains daily returns and risk-free rates\n",
    "return_rf_df = pd.merge(return_df, riskfree_df, left_index = True, right_index = True,how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72886fd-8095-42f9-b2a4-4ce8f38adeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = return_rf_df['Annual Riskfree Rate'].isna() & (return_rf_df.index < pd.Timestamp('2000-02-04'))\n",
    "return_rf_df.loc[mask, 'Annual Riskfree Rate'] = 0.03\n",
    "return_rf_df['Annual Riskfree Rate'] = return_rf_df['Annual Riskfree Rate'].ffill()\n",
    "return_rf_df['Daily Rf Rate'] = (1+return_rf_df['Annual Riskfree Rate'])**(1/255)-1\n",
    "return_rf_df['period_id'] = return_rf_df['Annual Riskfree Rate'].ne(return_rf_df['Annual Riskfree Rate'].shift()).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6e0f5-cb43-4d91-b622-165c1b4edb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for performance evaluation\n",
    "def performance_metrics(df):  \n",
    "    # Calculate drawdown: \n",
    "    cumulative_return = (1+df).cumprod()  # calculate portfolio cumulative return \n",
    "    running_max = cumulative_return.cummax()  # keep track of the highest cumulative return seen so far at each point in time\n",
    "    drawdown = (cumulative_return - running_max)/ running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    return drawdown, max_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774739c4-9548-4c18-813d-c8584777dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(df, rf):  # optimized portfolio return \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    rf.index = pd.to_datetime(rf.index)\n",
    "    rf_pfret = pd.merge(df,rf, left_index = True, right_index = True, how = 'left')\n",
    "    rf_pfret['period_id'] = rf_pfret.iloc[:,1].ne(pd.Series(rf_pfret.iloc[:,1]).shift()).cumsum()\n",
    "    sharpe_mp = {}\n",
    "    for pid, subdf in rf_pfret.groupby('period_id'):\n",
    "        r = subdf.iloc[:,0].mean()\n",
    "        stdeviat = subdf.iloc[:,0].std(ddof=0) \n",
    "        shr = (r-(subdf.iloc[:,1].mean())) /stdeviat\n",
    "        sharpe_mp[pid] = shr\n",
    "    rf_pfret['sharpe ratio'] = rf_pfret['period_id'].map(sharpe_mp)\n",
    "    return rf_pfret['sharpe ratio'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06e592-6106-4add-9e98-e9f8e8da27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortino_ratio(df, rf):  # optimized portfolio return \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    rf.index = pd.to_datetime(rf.index)\n",
    "    rf_pfret = pd.merge(df,rf, left_index = True, right_index = True, how = 'left')\n",
    "    rf_pfret['period_id'] = rf_pfret.iloc[:,1].ne(pd.Series(rf_pfret.iloc[:,1]).shift()).cumsum()\n",
    "    sortino_mp = {}\n",
    "    semideviatio_mp = {}\n",
    "    for pid, subdf in rf_pfret.groupby('period_id'):\n",
    "        r = subdf.iloc[:,0].mean()  # expected return of that subperiod \n",
    "        mar = subdf.iloc[:,1].mean()\n",
    "        below_mar = subdf.iloc[:,0][subdf.iloc[:,0] < mar]\n",
    "        semivario = (((mar-below_mar)**2).sum())/ len(subdf.iloc[:,0])\n",
    "        semideviatio = np.sqrt(semivario)\n",
    "        sortino_ratioo = (r-mar)/semideviatio\n",
    "        sortino_mp[pid] = sortino_ratioo\n",
    "        semideviatio_mp[pid] = semideviatio\n",
    "    rf_pfret['sortino ratio'] = rf_pfret['period_id'].map(sortino_mp)\n",
    "    rf_pfret['semi std'] = rf_pfret['period_id'].map(semideviatio_mp)\n",
    "    return rf_pfret['sortino ratio'].to_frame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0949551-3ac1-41a7-84b6-8073bc6061c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Sharpe and Sortino ratios and drawdowns for the MV portfolio\n",
    "MV_performance = performance_metrics(MV_ret_LP)\n",
    "MV_Sharpe = sharpe_ratio(MV_ret_LP, return_rf_df['Daily Rf Rate']).rename(columns={'sharpe ratio':'MV'})\n",
    "MV_Sortino = sortino_ratio(MV_ret_LP,return_rf_df['Daily Rf Rate']).rename(columns={'sortino ratio':'MV'})\n",
    "MV_Drawdown = MV_performance[0].rename(columns = {0:'MV'})\n",
    "MV_MDD = MV_performance[1]\n",
    "\n",
    "# calculate Sharpe and Sortino ratios and drawdowns for the DAX\n",
    "DAX_performance = performance_metrics(market_return)\n",
    "DAX_Sharpe= sharpe_ratio(market_return, return_rf_df['Daily Rf Rate']).rename(columns={'sharpe ratio':'DAX'})\n",
    "DAX_Sortino = sortino_ratio(market_return,return_rf_df['Daily Rf Rate']).rename(columns={'sortino ratio':'DAX'})\n",
    "DAX_Drawdown = DAX_performance[0].to_frame().rename(columns = {0:'DAX'})\n",
    "DAX_MDD = DAX_performance[1]\n",
    "\n",
    "color_map_perf = {'MV': 'red',\n",
    "            'DAX': 'blue'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94aaf9-3e60-409f-816b-71eb1bc83f2a",
   "metadata": {},
   "source": [
    "##### Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695778a1-3501-4848-ae27-b2362c3d48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Sharpe ratio for the MV portfolio and DAX\n",
    "Sharpe_comparison = pd.concat([MV_Sharpe, DAX_Sharpe], axis=1)\n",
    "px.line(Sharpe_comparison, x = Sharpe_comparison.index,\n",
    "                     y= Sharpe_comparison.columns, title = \"Figure 5: Sharpe Ratio\",\n",
    "                     labels = ({'value': 'Sharpe ratio'}), color_discrete_map = color_map_perf,\n",
    "                     height = 500, template = 'plotly_white').update_layout(legend_title = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a931a-be90-4c92-9f42-fbf6dbbf4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(Sharpe_comparison) # statistics on Sharpe ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdff95a-6f11-4c6a-b140-3d2a36c9336c",
   "metadata": {},
   "source": [
    "Note: \n",
    "- The MV portfolio has a higher average Sharpe ratio than the DAX. But this superiority comes at the cost of elevated volatility.\n",
    "- Both the MV portfolio and the DAX register extreme poor performance during the early 2000s due to the dotcom bubble burst.\n",
    "- The MV portfolio marks its maximum Sharpe ratio in April 2009, while the DAX reports its peak performance in June 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43141ac3-0b4d-46f3-a7a7-4cb294684c1f",
   "metadata": {},
   "source": [
    "##### Sortino Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ca8ad-de08-4fea-9d12-e9f888281492",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sortino_comparison = pd.concat([MV_Sortino, DAX_Sortino], axis=1)\n",
    "px.line(Sortino_comparison, x = Sortino_comparison.index,\n",
    "                     y= Sortino_comparison.columns, title = \"Figure 6: Sortino Ratio\",\n",
    "                     labels = ({'value': 'Sortino ratio'}),color_discrete_map = color_map_perf,\n",
    "                     height = 500, template = 'plotly_white').update_layout(legend_title = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd6f8f-8bca-4e41-8686-d4d651b71497",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(Sortino_comparison) # statistics on Sortino ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f2c51-ca36-4797-baf6-26fee259bd5a",
   "metadata": {},
   "source": [
    "Note: \n",
    "- In general, the Sortio ratio is higher than the Sharpe ratio. On average, the MV portfolio has a higher Sortino ratio than the DAX, which - however - comes at the cost of heightened uncertainty.\n",
    "- The MV portfolio records its highest Sortino ratio in June 2000, while the DAX - similarly to the Sharpe performnace - achieves its maximum in April 2025.\n",
    "- The minimum Sortino ratio occurs for both the MV portfolio and the DAX in September 2001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7acbc-3ebd-4c7c-88ac-d488f7370dce",
   "metadata": {},
   "source": [
    "#### Drawdown\n",
    "Understanding drawdown patterns provides long-term investors the ability to withstand them when they inevitably occur. A drawdown quantifies \n",
    "the loss an investment experiences from its historical peak. It can be expressed either in an absolute term or as a normalized drawdown, which represents the absolute loss in percentage. \n",
    "the absolute drawdown at time t is calculated as:  \n",
    "$$ D_t = M_t - V_t $$\n",
    "where $ùëÄ_ùë°$ denotes the maximum portfolio value observed up to time t and $ùëâ_ùë°$ is the portfolio value at time t. \n",
    "The normalized drawdown is defined as: \n",
    "$$\\bar{DD}_t = \\frac{M_t - V_t}{M_t}$$\n",
    "For model comparison, this study mainly used the concept of maximum drawdown $\\text{Max-DD}$, which describes the largest relative decline observed within a period $\\text{T}$ and is defined as the maximum of all normalized drawdowns in that period:  \n",
    "$$\\text{Max-DD} = \\max_{1 \\le t \\le T} \\bar{DD}_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e83aa4-bdcd-40fd-b7e2-658a349da425",
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_comparison = pd.concat([MV_Drawdown, DAX_Drawdown], axis=1)\n",
    "px.line(DD_comparison, x = DD_comparison.index, title = \"Figure 7: Drawdown\",\n",
    "        y = DD_comparison.columns, template = 'plotly_white',\n",
    "       labels = ({'value': '%'}), height = 500,color_discrete_map = color_map_perf).update_layout(legend_title = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef02a8-60ac-4cc9-b80b-ab7216940e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(DD_comparison)  # statistics on drawdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73153061-138b-4b5e-b964-2c26aab359e2",
   "metadata": {},
   "source": [
    "Note: \n",
    "- The drawdowns of the DAX generally have longer recovery times than the optimized portfolio. \n",
    "- The maximum drawdown of the MV portfolio occurs in November 2008 with a magnitude of approx. 70%. The drawdown takes the portfolio about 3 years to fully recover.\n",
    "- The DAX records in maximum drop in value in March 2003 with a magnitude of approx. 72%. Notably, this protracted drawdown requires a recovery time of more than 7 years. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
